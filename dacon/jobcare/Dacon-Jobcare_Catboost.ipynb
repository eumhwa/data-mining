{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Dacon-Jobcare_Catboost.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNyDpJ+S0gRdpLUVchzX2bO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"rziQJifqFnmm"},"outputs":[],"source":["import os, json\n","import random \n","from datetime import datetime\n","from typing import List ,Dict, Tuple\n","\n","import numpy as np\n","import pandas as pd\n","# from statsmodels.graphics.mosaicplot import mosaic\n","from matplotlib import pyplot as plt\n","\n","import torch\n","from catboost import Pool,CatBoostClassifier\n","\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import KFold\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.tree import export_text\n","\n","import optuna\n","from optuna import Trial, visualization\n","from optuna.samplers import TPESampler\n","\n","# print(f\"- os: {platform.platform()}\")\n","# print(f\"- python: {sys.version}\")\n","# print(f\"- pandas: {pd.__version__}\")\n","# print(f\"- numpy: {np.__version__}\")\n","# print(f\"- sklearn: {sklearn.__version__}\")"]},{"cell_type":"code","source":["data_path = \"/usr/src/coco/dacon/data/job_new\"\n","train_df = pd.read_csv(os.path.join(data_path, 'train.csv'))\n","test_df = pd.read_csv(os.path.join(data_path, 'test.csv'))\n","\n","code_d = pd.read_csv(os.path.join(data_path, '속성_D_코드.csv'))\n","code_h = pd.read_csv(os.path.join(data_path, '속성_H_코드.csv'))\n","code_l = pd.read_csv(os.path.join(data_path, '속성_L_코드.csv'))\n","\n","submission_df = pd.read_csv(os.path.join(data_path, 'sample_submission.csv'))"],"metadata":{"id":"4ycu_gH8FoMu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["code_d.columns= [\"attribute_d\",\"attribute_d_d\",\"attribute_d_s\",\"attribute_d_m\",\"attribute_d_l\"]\n","code_h.columns= [\"attribute_h\",\"attribute_h_m\",\"attribute_h_l\"]\n","code_l.columns= [\"attribute_l\",\"attribute_l_d\",\"attribute_l_s\",\"attribute_l_m\",\"attribute_l_l\"]\n","\n","print(train_df.shape, test_df.shape, code_d.shape, code_h.shape, code_l.shape)\n","# train_df.head()"],"metadata":{"id":"kISggW_eFoPk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seed_no = 1000\n","\n","def seed_everything(seed): # seed 고정\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","    random.seed(seed)\n","\n","def merge_codes(df:pd.DataFrame, df_code:pd.DataFrame, col:str) -> pd.DataFrame:\n","    df = df.copy()\n","    df_code = df_code.copy()\n","    df_code = df_code.add_prefix(f\"{col}_\")\n","    df_code.columns.values[0] = col\n","    return pd.merge(df, df_code, how=\"left\", on=col)\n","\n","\n","def preprocess_data(\n","    df:pd.DataFrame, is_train:bool=True, \n","    cols_merge:List[Tuple[str,pd.DataFrame]] = [], \n","    cols_equi:List[Tuple[str,str]]= [] ,\n","    cols_drop:List[str] = [\"id\", \"person_prefer_f\", \"person_prefer_g\", \"contents_open_dt\"]\n","    )->Tuple[pd.DataFrame,np.ndarray]:\n","    \n","    df = df.copy()\n","\n","    y_data = None\n","    if is_train:\n","        y_data = df[\"target\"].to_numpy()\n","        df = df.drop(columns=\"target\")\n","\n","    for col, df_code in cols_merge:\n","        df = merge_codes(df, df_code, col)\n","\n","    cols = df.select_dtypes(bool).columns.tolist()\n","    df[cols] = df[cols].astype(int)\n","\n","    for col1, col2 in cols_equi:\n","        df[f\"{col1}_{col2}\"] = (df[col1] == df[col2] ).astype(int)\n","\n","    df = df.drop(columns=cols_drop)\n","    return (df , y_data)\n","\n","# 소분류 중분류 대분류 속성코드 merge 컬럼명 및 데이터 프레임 리스트\n","cols_merge = [\n","              (\"person_prefer_d_1\" , code_d),\n","              (\"person_prefer_d_2\" , code_d),\n","              (\"person_prefer_d_3\" , code_d),\n","              (\"contents_attribute_d\" , code_d),\n","              (\"person_prefer_h_1\" , code_h),\n","              (\"person_prefer_h_2\" , code_h),\n","              (\"person_prefer_h_3\" , code_h),\n","              (\"contents_attribute_h\" , code_h),\n","              (\"contents_attribute_l\" , code_l),\n","]\n","\n","# 회원 속성과 콘텐츠 속성의 동일한 코드 여부에 대한 컬럼명 리스트\n","cols_equi = [\n","    (\"contents_attribute_c\", \"person_prefer_c\"),\n","    (\"contents_attribute_e\", \"person_prefer_e\"),\n","\n","    (\"person_prefer_d_2_attribute_d_s\" , \"contents_attribute_d_attribute_d_s\"),\n","    (\"person_prefer_d_2_attribute_d_m\" , \"contents_attribute_d_attribute_d_m\"),\n","    (\"person_prefer_d_2_attribute_d_l\" , \"contents_attribute_d_attribute_d_l\"),\n","    (\"person_prefer_d_3_attribute_d_s\" , \"contents_attribute_d_attribute_d_s\"),\n","    (\"person_prefer_d_3_attribute_d_m\" , \"contents_attribute_d_attribute_d_m\"),\n","    (\"person_prefer_d_3_attribute_d_l\" , \"contents_attribute_d_attribute_d_l\"),\n","    \n","    (\"person_prefer_h_1_attribute_h_m\" , \"contents_attribute_h_attribute_h_m\"),\n","    (\"person_prefer_h_1_attribute_h_l\" , \"contents_attribute_h_attribute_h_l\"),\n","    (\"person_prefer_h_2_attribute_h_m\" , \"contents_attribute_h_attribute_h_m\"),\n","    (\"person_prefer_h_2_attribute_h_l\" , \"contents_attribute_h_attribute_h_l\"),\n","    (\"person_prefer_h_3_attribute_h_m\" , \"contents_attribute_h_attribute_h_m\"),\n","    (\"person_prefer_h_3_attribute_h_l\" , \"contents_attribute_h_attribute_h_l\"),\n","    \n","    # (\"person_prefer_h_1_attribute_h_p\" , \"contents_attribute_h_attribute_h_p\"),\n","    # (\"person_prefer_h_2_attribute_h_p\" , \"contents_attribute_h_attribute_h_p\"),\n","    # (\"person_prefer_h_3_attribute_h_p\" , \"contents_attribute_h_attribute_h_p\"),\n","]\n","\n","def split_data(x_train, y_train, idx):\n","    X_val = x_train.iloc[idx, ].values\n","    Y_val = y_train[idx]\n","\n","    X_train = x_train.loc[~x_train.index.isin(idx)].values\n","    mask = np.ones(y_train.size, dtype=bool)\n","    mask[idx] = False\n","    Y_train = y_train[mask]\n","\n","    print(X_train.shape, X_val.shape)\n","    print(Y_train.shape, Y_val.shape)\n","    return X_train, Y_train, X_val, Y_val\n","\n","def get_thr(score, gt, n_iter=10000):\n","    f1s = []\n","    for i in range(1, n_iter):\n","        tmp_cutoff = i / n_iter\n","        f1 = f1_score(gt, (score[:, 1]>=tmp_cutoff)*1)\n","        f1s.append(f1)\n","    return f1s.index(max(f1s)) / n_iter, max(f1s)\n","\n","# class F1_Score(Metric):\n","#     def __init__(self):\n","#         self._name = \"f1\"\n","#         self._maximize = True\n","\n","#     def __call__(self, y_true, y_score):\n","#         score = f1_score(y_true, (y_score[:, 1]>0.5)*1)\n","#         return score\n","\n","\n","def get_new_category(X, col, nonzero_importance):\n","    new_cat = np.zeros_like(X[col])\n","    for i, k in enumerate(X[col]):\n","        try:\n","            new_cat[i] = nonzero_importance[k] + 1\n","        except:\n","            continue\n","    return new_cat\n","            \n","            \n","def merge_categories(x_train, x_test, y_train, col, max_depth=10):\n","#     print(len(x_train[col].unique()))\n","    _x_train, _x_test = x_train.copy(), x_test.copy()\n","    enc = OneHotEncoder()\n","    x = _x_train[col].values.reshape(-1, 1)\n","    enc.fit(x)\n","    x = enc.transform(x).toarray()\n","            \n","    dt = DecisionTreeClassifier(random_state=seed_no, max_depth=max_depth)\n","    dt = dt.fit(x, y_train)\n","    nonzero_importance = np.where(dt.feature_importances_ != 0)[0]\n","    print(nonzero_importance)\n","    nonzero_importance = dict(zip(nonzero_importance, range(len(nonzero_importance))))\n","#     print(nonzero_importance)\n","    \n","    _x_train[col] = get_new_category(_x_train, col, nonzero_importance)\n","    _x_test[col] = get_new_category(_x_test, col, nonzero_importance)\n","        \n","    return _x_train, _x_test\n","\n","seed_everything(seed_no)"],"metadata":{"id":"tUFWZYxHFoR_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습에 필요없는 컬럼 리스트\n","cols_drop = [\n","    \"id\",\n","    \"person_prefer_f\", #only one value\n","    \"person_prefer_g\", #only one value\n","    \"contents_open_dt\",\n","    \"person_rn\",\n","    \"contents_rn\"]\n","try: \n","    train_df = train_df.drop(columns=['contents_open_mm']) #mosaic plot\n","except:\n","    print(\"already removed col: contents_open_mm\")\n","\n","x_train, y_train = preprocess_data(train_df, cols_merge = cols_merge , cols_equi= cols_equi , cols_drop = cols_drop)\n","x_test, _ = preprocess_data(test_df,is_train = False, cols_merge = cols_merge , cols_equi= cols_equi  , cols_drop = cols_drop)\n","x_train.shape , y_train.shape , x_test.shape"],"metadata":{"id":"h6GTjJJHFoUh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["merge_cat = False\n","cutoff = 10\n","\n","cat_idxs = []\n","cat_dims = []\n","ordinal_col = ['person_attribute_a_1', 'person_attribute_b', 'person_prefer_e', 'contents_attribute_e']\n","for idx, col in enumerate(x_train.columns):\n","    # if 'match' not in col or col not in ordinal_col:\n","    if col not in ordinal_col:\n","        n_cat = len(x_train[col].unique())\n","        if n_cat > cutoff and merge_cat:\n","            x_train, x_test = merge_categories(x_train, x_test, y_train, col, max_depth=cutoff)\n","            \n","        le = LabelEncoder()\n","        le.fit(x_train[col].values)\n","        le_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n","\n","        x_train[col] = x_train[col].apply(lambda x: le_dict.get(x, len(le_dict)))\n","        x_test[col] = x_test[col].apply(lambda x: le_dict.get(x, len(le_dict)))\n","        \n","        cat_idxs.append(idx)\n","        cat_dims.append(len(le_dict)+1)\n","    \n","    if col in ordinal_col:\n","        x_train[col] = x_train[col] / max(x_train[col])\n","        x_test[col] = x_test[col] / max(x_test[col])"],"metadata":{"id":"4yhN2Fq1FoXc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(x_train.shape , y_train.shape , x_test.shape)\n","x_train[ordinal_col].describe()"],"metadata":{"id":"V6wqD6xXFoZf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Catboost"],"metadata":{"id":"qaXKG_EEFyND"}},{"cell_type":"code","source":["cat_features = x_train.columns[cat_idxs].tolist()"],"metadata":{"id":"WI6S1mmLFobX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_fold=5\n","n_round = 2000\n","one_hot_max_size = 5\n","depth = 5\n","patience = 300"],"metadata":{"id":"CipKFfrHFodI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["kf = KFold(n_splits=n_fold, random_state=seed_no, shuffle=True)\n","\n","CV_f1, CV_info = [], []\n","fold_no = 1\n","for train_index, test_index in kf.split(x_train.values):\n","    train_X, val_X = x_train.iloc[train_index], x_train.iloc[test_index]\n","    train_Y, val_Y = y_train[train_index], y_train[test_index]\n","    \n","    model = CatBoostClassifier(\n","        iterations=n_round, \n","        random_state=seed_no, \n","        task_type=\"GPU\",\n","        eval_metric=\"F1\", \n","        cat_features=cat_features, \n","        depth=depth,\n","        one_hot_max_size=one_hot_max_size)\n","    \n","    model.fit(train_X, train_Y, \n","            eval_set=[(val_X, val_Y)], \n","            early_stopping_rounds=patience ,\n","            verbose = 100\n","        )\n","    \n","    CV_f1.append(model.get_best_score())\n","    \n","    val_preds = model.predict_proba(val_X)\n","    best_thr = get_thr(val_preds, val_Y)\n","    print(f'best threshold and f1 score from fold {fold_no}-th validation set : {best_thr}')\n","    \n","    test_preds = model.predict_proba(x_test)\n","    submission_df[f'target_prob_cb_{fold_no}'] = test_preds[:,1]\n","    test_preds = (test_preds[:,1] >= best_thr[0]) * 1\n","    submission_df[f'target_cb_{fold_no}'] = test_preds\n","    \n","    CV_info.append({'fold':fold_no, 'val_threshold':best_thr[0], 'val_f1':best_thr[1]})\n","    # ckpt_path = os.path.join(save_dir, f\"fold_{fold_no}\")\n","    # final_Tabnet.save_model(ckpt_path)\n","    fold_no +=1\n","    \n"],"metadata":{"id":"iG4RYObgFofg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["avg = np.mean([c[\"validation\"][\"F1\"] for c in CV_f1])\n","print(CV_f1)\n","print(avg)"],"metadata":{"id":"S-ejGVU5FoiJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"m3Rhr5gIFoj4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission_df.head()"],"metadata":{"id":"2zPnt4QpFomB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rowsum = submission_df[[f\"target_cb_{i}\" for i in range(1, n_fold+1)]].sum(axis=1)\n","sum(rowsum==n_fold/2)"],"metadata":{"id":"CBpMrPf9FooJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cutoff = n_fold/2\n","# cutoff = 3\n","\n","binary_result = []\n","for i, r in enumerate(rowsum):\n","    if r > cutoff:\n","        binary_result.append(1)\n","    elif r <= cutoff:\n","        binary_result.append(0)"],"metadata":{"id":"NSf7E9nDF6E7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(binary_result[:10])"],"metadata":{"id":"2QIYZ8MJF6HD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission_df[\"target\"] = binary_result\n","res_df = submission_df[[\"id\", \"target\"]]\n","res_df.describe()"],"metadata":{"id":"i3kvib_VF6Je"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["res_df.to_csv(f'{data_path}/results/cb8.csv', index=False)"],"metadata":{"id":"lzq6B-K7F6Lp"},"execution_count":null,"outputs":[]}]}