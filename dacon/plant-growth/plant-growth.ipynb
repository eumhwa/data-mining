{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Dacon-생육기간.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyONA7mjhydkiPjjOwVqkKCV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"77aFvVbVIvYB"},"outputs":[],"source":["import os\n","import json\n","import random\n","import time\n","import copy\n","import gc\n","from glob import glob\n","from tqdm.auto import tqdm\n","# from tqdm import tqdm\n","\n","import pandas as pd\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch import optim\n","from torch import nn\n","from torch.optim import lr_scheduler\n","from torchvision.transforms import ToTensor\n","from torchvision import transforms\n","import torchvision.models as models\n","\n","!pip install transformers\n","from transformers import ViTFeatureExtractor, ViTModel, ViTForImageClassification, ViTConfig"]},{"cell_type":"code","source":["def extract_day(file_name):\n","    day = int(file_name.split('.')[-2][-2:])\n","    return day\n","\n","\n","def make_day_array(image_pathes):\n","    day_array = np.array([extract_day(file_name) for file_name in image_pathes])\n","    return day_array\n","\n","\n","def make_image_path_array(root_path=None):\n","    if root_path is None:\n","        bc_directories = glob('./BC/*')\n","        lt_directories = glob('./LT/*')\n","\n","    else:\n","        bc_directories = glob(root_path + 'BC/*')\n","        lt_directories = glob(root_path + 'LT/*')\n","\n","    bc_image_path = []\n","    for bc_path in bc_directories:\n","        images = glob(bc_path + '/*.png')\n","        bc_image_path.extend(images)\n","\n","    lt_image_path = []\n","    for lt_path in lt_directories:\n","        images = glob(lt_path + '/*.png')\n","        lt_image_path.extend(images)\n","\n","    return bc_image_path, lt_image_path\n","\n","\n","def make_dataframe(root_path=None):\n","    bc_image_path, lt_image_path = make_image_path_array(root_path)\n","    bc_day_array = make_day_array(bc_image_path)\n","    lt_day_array = make_day_array(lt_image_path)\n","\n","    bc_df = pd.DataFrame({'file_name': bc_image_path,\n","                          'day': bc_day_array})\n","    bc_df['species'] = 'bc'\n","\n","    lt_df = pd.DataFrame({'file_name': lt_image_path,\n","                          'day': lt_day_array})\n","    lt_df['species'] = 'lt'\n","\n","    total_data_frame = pd.concat([bc_df, lt_df]).reset_index(drop=True)\n","\n","    return total_data_frame\n","\n","def make_combination(species, data_frame):\n","    before_file_path = []\n","    after_file_path = []\n","    time_delta = []\n","\n","    for version in data_frame[data_frame['species'] == species]['version'].unique():\n","        for i in range(0, len(data_frame[data_frame['version'] == version]) - 1):\n","            for j in range(i + 1, len(data_frame[data_frame['version'] == version])):\n","                after = data_frame[data_frame['version'] == version].iloc[j].reset_index(drop=True)\n","                before = data_frame[data_frame['version'] == version].iloc[i].reset_index(drop=True)\n","\n","                before_file_path.append(before[0])\n","                after_file_path.append(after[0])\n","\n","                delta = int(after[1] - before[1])\n","                time_delta.append(delta)\n","\n","\n","    combination_df = pd.DataFrame({\n","        'before_file_path': before_file_path,\n","        'after_file_path': after_file_path,\n","        'time_delta': time_delta,\n","    })\n","\n","    combination_df['species'] = species\n","\n","    return combination_df\n","\n","class KistDataset(Dataset):\n","    def __init__(self, combination_df, transform, is_test= None):\n","        self.combination_df = combination_df\n","        self.transform = transform\n","        self.is_test = is_test\n","\n","    def __getitem__(self, idx):\n","        before_image = Image.open(self.combination_df.iloc[idx]['before_file_path'])\n","        after_image = Image.open(self.combination_df.iloc[idx]['after_file_path'])\n","\n","        before_image = self.transform(before_image)\n","        after_image = self.transform(after_image)\n","        if self.is_test:\n","            return before_image, after_image\n","        time_delta = self.combination_df.iloc[idx]['time_delta']\n","        return before_image, after_image, time_delta\n","\n","    def __len__(self):\n","        return len(self.combination_df)\n","    \n","    \n","    \n","def seed_everything(seed): # seed 고정\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","    random.seed(seed)\n","\n","\n","seed_everything(1000)"],"metadata":{"id":"lLCUMn_AIyHw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["root_path = \"/usr/src/coco/dacon/data/open/train_dataset/\"\n","\n","bt_direct = glob(root_path + '/BC/*')\n","bt_direct_name = [x[-5:] for x in bt_direct]\n","lt_direct = glob(root_path + '/LT/*')\n","lt_direct_name = [x[-5:] for x in lt_direct]\n","\n","bt_images = {key: glob(name + '/*.png') for key, name in zip(bt_direct_name, bt_direct)}\n","lt_images = {key: glob(name + '/*.png') for key, name in zip(lt_direct_name, lt_direct)}\n","\n","bt_dayes = {key: make_day_array(bt_images[key]) for key in bt_direct_name}\n","lt_dayes = {key: make_day_array(lt_images[key]) for key in lt_direct_name}\n","\n","bt_dfs = []\n","\n","for i in bt_direct_name:\n","    bt_df = pd.DataFrame({\n","        'file_name': bt_images[i],\n","        'day': bt_dayes[i],\n","        'species': 'bc',\n","        'version': i\n","    })\n","    bt_dfs.append(bt_df)\n","    \n","lt_dfs = []\n","\n","for i in lt_direct_name:\n","    lt_df = pd.DataFrame({\n","        'file_name': lt_images[i],\n","        'day': lt_dayes[i],\n","        'species': 'lt',\n","        'version': i\n","    })\n","    lt_dfs.append(lt_df)\n","\n","bf_dataframe = pd.concat(bt_dfs).reset_index(drop=True)\n","lt_dataframe = pd.concat(lt_dfs).reset_index(drop=True)\n","\n","total_dataframe = pd.concat([bf_dataframe, lt_dataframe]).reset_index(drop=True)\n","total_dataframe"],"metadata":{"id":"Vqx2L9WTIyJv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bt_combination = make_combination('bc', total_dataframe)\n","lt_combination = make_combination('lt', total_dataframe)\n","\n","print(bt_combination.shape, lt_combination.shape)"],"metadata":{"id":"2LgGlee6IyL_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bt_train = bt_combination\n","lt_train = lt_combination\n","\n","train_set = pd.concat([bt_train, lt_train]).reset_index(drop=True)\n","train_set"],"metadata":{"id":"fxWcUTOhIyOA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tds = train_set.time_delta.unique()\n","split_ratio = 0.9\n","train_idx, valid_idx = [], []\n","for d in tds:\n","    total_idx = train_set[train_set.time_delta==d].index.values\n","    tmp_train_idx = np.random.choice(total_idx, int(len(total_idx)*split_ratio), replace=False).tolist()\n","    train_idx.extend(tmp_train_idx)\n","    valid_idx.extend([t for t in total_idx if t not in tmp_train_idx])\n","    print(f\"time_delta :{d} and added train samples: {len(tmp_train_idx)} / valid samples: {len(total_idx) - len(tmp_train_idx)}\")"],"metadata":{"id":"SBh79mmTIyQH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(train_idx), len(valid_idx))"],"metadata":{"id":"BJgnfoR9IySM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df = train_set.loc[train_idx].reset_index(drop=True)\n","valid_df = train_set.loc[valid_idx].reset_index(drop=True)\n","print(train_df.shape, valid_df.shape)"],"metadata":{"id":"qRER_dDkIyUV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_size = (224, 224)\n","train_t = transforms.Compose([\n","            transforms.Resize(image_size),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomResizedCrop(\n","                image_size, scale=(0.95, 1), ratio=(0.95, 1)\n","            ),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ])\n","valid_t = transforms.Compose([\n","            transforms.Resize(image_size),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ])\n","\n","train_dataset = KistDataset(train_df, train_t)\n","valid_dataset = KistDataset(valid_df, valid_t)"],"metadata":{"id":"X2F18jqrIyWp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pretrained_architecture = \"google/vit-base-patch16-224-in21k\" #'google/vit-base-patch16-224'\n","\n","class UnitViTModel(nn.Module):\n","\n","    def __init__(self):\n","        super(UnitViTModel, self).__init__()\n","        # self.feature_extractor = ViTFeatureExtractor.from_pretrained(pretrained_architecture)\n","        self.vit = ViTModel.from_pretrained(pretrained_architecture, output_attentions=False, add_pooling_layer=False)\n","        \n","    def forward(self, input):\n","        outputs = self.vit(input)\n","        seq_output = outputs[0][:, 0, :]\n","        return seq_output\n","\n","\n","class CompareNet(nn.Module):\n","\n","    def __init__(self, vit_out_dim=768, num_classes=1):\n","        super(CompareNet, self).__init__()\n","        self.before_net = UnitViTModel()\n","        self.after_net = UnitViTModel()\n","        self.fc1 = nn.Linear(2*vit_out_dim, vit_out_dim)\n","        self.fc2 = nn.Linear(vit_out_dim, num_classes)\n","        \n","    def forward(self, before_input, after_input):\n","        before_seq = self.before_net(before_input)\n","        after_seq = self.after_net(after_input)\n","        seqs = torch.concat([before_seq, after_seq], axis=1)\n","        \n","        delta = self.fc1(seqs)\n","        delta = self.fc2(delta)\n","      \n","        return delta\n","    \n","    \n","class UnitViTModel_v2(nn.Module):\n","\n","    def __init__(self):\n","        super(UnitViTModel_v2, self).__init__()\n","        # self.feature_extractor = ViTFeatureExtractor.from_pretrained(pretrained_architecture)\n","        self.vit = ViTModel.from_pretrained(pretrained_architecture, output_attentions=False, add_pooling_layer=False)\n","        \n","    def forward(self, input):\n","        outputs = self.vit(input)\n","        seq_output = outputs[0][:, 0, :]\n","        return seq_output\n","\n","\n","class CompareNet_v2(nn.Module):\n","\n","    def __init__(self, vit_out_dim=768, d_model=512, nhead=4, nlayers=4, num_classes=1):\n","        super(CompareNet_v2, self).__init__()\n","        self.vit_encoder = UnitViTModel_v2()\n","        self.pooling = nn.AvgPool1d(kernel_size=3, stride=3)\n","        self.layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n","        self.transformer = nn.TransformerEncoder(self.layers, num_layers=nlayers)\n","        \n","        self.fc = nn.Linear(d_model, num_classes)\n","        self.fc1 = nn.Linear(2*vit_out_dim, vit_out_dim)\n","        self.fc2 = nn.Linear(vit_out_dim, num_classes)\n","        \n","    def forward(self, before_input, after_input):\n","        before_seq = self.vit_encoder(before_input)\n","        before_seq = self.pooling(before_seq)\n","        \n","        after_seq = self.vit_encoder(after_input)\n","        after_seq = self.pooling(after_seq)\n","        \n","        seqs = torch.concat([before_seq, after_seq], axis=1).unsqueeze(1)\n","        seqs = self.transformer(seqs)\n","        \n","        delta = self.fc(seqs.squeeze(1))\n","        #delta = self.fc2(delta)\n","      \n","        return delta\n","    \n","    \n","class CompareNet_v3(nn.Module):\n","\n","    def __init__(self, vit_out_dim=768, d_model=128, nhead=4, nlayers=2, num_classes=1):\n","        super(CompareNet_v3, self).__init__()\n","        self.vit_encoder = UnitViTModel_v2()\n","        self.pooling = nn.AvgPool1d(kernel_size=3, stride=2)\n","        self.encoder = nn.Linear(1, d_model)\n","        \n","        self.layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n","        self.transformer = nn.TransformerEncoder(self.layers, num_layers=nlayers)\n","        \n","        self.fc = nn.Linear((vit_out_dim-2), num_classes)\n","        \n","        \n","    def forward(self, before_input, after_input):\n","        before_seq = self.vit_encoder(before_input) #(B, 768)\n","        before_seq = self.pooling(before_seq) # (B, 383)\n","        \n","        after_seq = self.vit_encoder(after_input)\n","        after_seq = self.pooling(after_seq)\n","        \n","        seqs = torch.concat([before_seq, after_seq], axis=1).unsqueeze(2) #(B, 766, 1)\n","        seqs = self.encoder(seqs) # (B, 766, d_model=128)\n","        seqs = self.transformer(seqs)[:, :, 0] #(B, 766, d_model=128) -> (B, 766, 1)\n","        \n","        delta = self.fc(seqs) #(B, 1)\n","        #delta = self.fc2(delta)\n","      \n","        return delta"],"metadata":{"id":"p5uXuvDLIyYw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"aic90PArJgh4"}},{"cell_type":"code","source":["lr = 3e-5\n","epochs = 30\n","batch_size = 8\n","valid_batch_size = 64\n","device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n","\n","# model = CompareNet().to(device)\n","# model = CompareNet_v2().to(device)\n","model = CompareNet_v3().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","criterion = nn.MSELoss()"],"metadata":{"id":"nuy0HEigIya2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data_loader = DataLoader(train_dataset,\n","                               batch_size=batch_size,\n","                               shuffle=True)\n","\n","valid_data_loader = DataLoader(valid_dataset,\n","                               batch_size=valid_batch_size)\n","\n","print(len(train_data_loader), len(valid_data_loader))"],"metadata":{"id":"oxjSJYNLIyc6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import copy\n","\n","best_val_loss, base_model_wts = 10e5, None\n","progress = tqdm(range(epochs))\n","for epoch in progress:\n","    for step, (before_image, after_image, time_delta) in tqdm(enumerate(train_data_loader)):\n","        before_image = before_image.to(device)\n","        after_image = after_image.to(device)\n","        time_delta = time_delta.to(device)\n","\n","        optimizer.zero_grad()\n","        logit = model(before_image, after_image)\n","\n","        #train_loss = (torch.sum(torch.abs(logit.squeeze(1).float() - time_delta.float())) /\n","        #              torch.LongTensor([batch_size]).squeeze(0).to(device))\n","        train_loss = criterion(logit.squeeze(1), time_delta.float())\n","        \n","        train_loss.backward()\n","        optimizer.step()\n","        if step % 100 == 0:\n","            print(f'[Epoch-step]: {epoch}-{step}, training loss : {train_loss.detach().cpu().numpy()}')     \n","        # progress.set_description(f\"training loss: {train_loss.detach().cpu().numpy()}\")\n","\n","    valid_losses = []\n","    with torch.no_grad():\n","        for valid_before, valid_after, time_delta in tqdm(valid_data_loader):\n","            valid_before = valid_before.to(device)\n","            valid_after = valid_after.to(device)\n","            valid_time_delta = time_delta.to(device)\n","\n","\n","            logit = model(valid_before, valid_after)\n","            #valid_loss = (torch.sum(torch.abs(logit.squeeze(1).float() - valid_time_delta.float())) /\n","            #              torch.LongTensor([valid_batch_size]).squeeze(0).to(device))\n","            valid_loss = criterion(logit.squeeze(1), valid_time_delta.float())\n","            valid_losses.append(valid_loss.detach().cpu())\n","\n","    \n","    cur_val_loss = sum(valid_losses)/len(valid_losses)\n","    print(f'#### VALIDATION_LOSS : {cur_val_loss} ####')\n","    if cur_val_loss < best_val_loss:\n","        print(\"validation loss updated!\")\n","        best_val_loss = cur_val_loss\n","        best_model_wts = copy.deepcopy(model.state_dict())\n","        ckpt = {\n","            'model': best_model_wts,\n","            'best_epoch': epoch,\n","            'best_val_loss': best_val_loss\n","\n","        }\n","        torch.save(ckpt, 'v3_best_211212.pt')\n","        print(f'ckpt saved!')\n"],"metadata":{"id":"XE4AP9T5IyfM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"hIVtt0ZLIyhh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"-v2penikIyjb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Test"],"metadata":{"id":"Ef-vep5IJdFM"}},{"cell_type":"code","source":["model_ckpt = torch.load(\"/usr/src/coco/dacon/v3_best_211212.pt\")"],"metadata":{"id":"tJ_SNpW-Iylb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# model = CompareNet().to(device)\n","# model = CompareNet_v2().to(device)\n","model = CompareNet_v3().to(device)\n","# model = CompareNet_v3(d_model=256, nlayers=4).to(device)"],"metadata":{"id":"R7MaarXsIynn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.load_state_dict(model_ckpt[\"model\"])\n","model.to(device)\n","print(f\"best epoch: {model_ckpt['best_epoch']}, / best validation loss: {model_ckpt['best_val_loss'].numpy()}\")"],"metadata":{"id":"i9g0FPMyIyp0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_size = (224, 224)\n","\n","test_t = transforms.Compose([\n","            transforms.Resize(image_size),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ])"],"metadata":{"id":"zTx_O3Y_IysE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_csv_path = \"/usr/src/coco/dacon/data/open/test_dataset/test_data.csv\"\n","\n","test_set = pd.read_csv(test_csv_path)\n","test_set['l_root'] = test_set['before_file_path'].map(lambda x: '/usr/src/coco/dacon/data/open/test_dataset/' + x.split('_')[1] + '/' + x.split('_')[2])\n","test_set['r_root'] = test_set['after_file_path'].map(lambda x: '/usr/src/coco/dacon/data/open/test_dataset/' + x.split('_')[1] + '/' + x.split('_')[2])\n","test_set['l_path'] = test_set['l_root'] + '/' + test_set['before_file_path'] + '.png'\n","test_set['r_path'] = test_set['r_root'] + '/' + test_set['after_file_path'] + '.png'\n","test_set['before_file_path'] = test_set['l_path']\n","test_set['after_file_path'] = test_set['r_path']\n","print(test_set.columns)\n","print(test_set.iloc[:1, :].values)"],"metadata":{"id":"tzaXKEL_JnI8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataset = KistDataset(test_set, test_t, is_test=True)\n","test_data_loader = DataLoader(test_dataset, batch_size=64)\n","\n","test_value = []\n","model.eval()\n","with torch.no_grad():\n","    for test_before, test_after in tqdm(test_data_loader):\n","        test_before = test_before.to(device)\n","        test_after = test_after.to(device)\n","        logit = model(test_before, test_after)\n","        value = logit.squeeze(1).detach().cpu().float()\n","        \n","        test_value.extend(value)"],"metadata":{"id":"xNLqIdCTJnLK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission = pd.read_csv('/usr/src/coco/dacon/data/open/sample_submission.csv')\n","submission.head()"],"metadata":{"id":"TjmNXJgHJr3x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_sub = torch.FloatTensor(test_value)\n","\n","__sub = _sub.numpy()\n","__sub[np.where(__sub<1)] = 1\n","\n","submission['time_delta'] = __sub\n","submission.to_csv('result_v3_ep21.csv', index=False)"],"metadata":{"id":"UdPli2giJr8g"},"execution_count":null,"outputs":[]}]}