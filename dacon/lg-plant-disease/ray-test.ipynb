{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ray-test.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM4RUZg/neaDEUDTB3/+kTz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wdXfpVVXEgpT"},"outputs":[],"source":["import os\n","from typing import Dict\n","\n","import torch\n","import torch.nn.functional as F\n","\n","import ray\n","import ray.train as train\n","from ray.train.trainer import Trainer\n","from ray.train.callbacks import JsonLoggerCallback\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor"]},{"cell_type":"code","source":["training_data = datasets.FashionMNIST(\n","    root=\"~/data\",\n","    train=True,\n","    download=True,\n","    transform=ToTensor(),\n",")\n","# Download test data from open datasets.\n","test_data = datasets.FashionMNIST(\n","    root=\"~/data\",\n","    train=False,\n","    download=True,\n","    transform=ToTensor(),\n",")"],"metadata":{"id":"6M088XxGE84V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define model-1\n","class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super(NeuralNetwork, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(28 * 28, 512), nn.ReLU(), nn.Linear(512, 512), nn.ReLU(),\n","            nn.Linear(512, 10), nn.ReLU())\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits "],"metadata":{"id":"WTh66m-3E86w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define model-2\n","class Classifier(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.fc1 = nn.Linear(784, 120)\n","        self.fc2 = nn.Linear(120, 120)\n","        self.fc3 = nn.Linear(120,10)\n","        self.dropout = nn.Dropout(0.2)\n","\n","    def forward(self,x):\n","        x = x.view(x.shape[0],-1)\n","        x = self.dropout(F.relu(self.fc1(x)))\n","        x = self.dropout(F.relu(self.fc2(x)))\n","        x = F.log_softmax(self.fc3(x), dim=1)\n","        return x"],"metadata":{"id":"kRB2mJZWE89_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define accuracy function\n","def accuracy_fn(y_pred, y_true):\n","    n_correct = torch.eq(y_pred, y_true).sum().item()\n","    acc = (n_correct / len(y_pred)) * 100\n","    return acc"],"metadata":{"id":"U8qOT5HmE8_m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_epoch(dataloader, model, loss_fn, optimizer, epoch):\n","    size = len(dataloader.dataset)\n","    for batch, (X, y) in enumerate(dataloader):\n","        # Compute prediction error\n","        pred = model(X)\n","        loss = loss_fn(pred, y)\n","        \n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()"],"metadata":{"id":"jv1ZGbe5E9BW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def validate_epoch(dataloader, model, loss_fn, epoch):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    model.eval()\n","    test_loss, correct, acc =  0, 0, 0.0\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            pred = model(X)\n","            test_loss += loss_fn(pred, y).item()\n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","            predictions = pred.max(dim=1)[1]\n","            acc += accuracy_fn(predictions, y)\n","    test_loss /= num_batches\n","    acc /= num_batches\n","    correct /= size\n","    if epoch > 0 and epoch % 50 == 0:\n","        print(f\"Epoc: {epoch}, Avg validation loss: {test_loss:.2f}, Avg validation accuracy: {acc:.2f}%\") \n","        print(\"--\" * 40)\n","    return test_loss"],"metadata":{"id":"PWkPILSzE9D4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_func(config: Dict):\n","    batch_size = config.get(\"batch_size\", 64) \n","    lr = config.get('lr', 1e-3)\n","    epochs = config.get(\"epochs\", 20)\n","    momentum = config.get(\"momentum\", 0.9)\n","    model_type = config.get('model_type', None)\n","    loss_fn = config.get(\"loss_fn\", nn.NLLLoss())\n","\n","    # Create data loaders.\n","    train_dataloader = DataLoader(training_data, batch_size=batch_size)\n","    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n","\n","    # Prepare to use Ray integrated wrappers around PyTorch's Dataloaders\n","    train_dataloader = train.torch.prepare_data_loader(train_dataloader)\n","    test_dataloader = train.torch.prepare_data_loader(test_dataloader)\n","\n","    # Create model.\n","\n","    model = Classifier() if model_type else NeuralNetwork()\n","    # Prepare to use Ray integrated wrappers around PyTorch's model\n","    model = train.torch.prepare_model(model)\n","    \n","    # Get or objective loss function\n","    loss_fn = config.get(\"loss_fn\", nn.NLLLoss())\n","\n","    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n","\n","    loss_results = []\n","\n","    for e in range(epochs):\n","        train_epoch(train_dataloader, model, loss_fn, optimizer, e)\n","        loss = validate_epoch(test_dataloader, model, loss_fn, e)\n","        train.report(loss=loss)\n","        loss_results.append(loss)\n","\n","    return loss_results"],"metadata":{"id":"NicYBiy7E9PZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_fashion_mnist(num_workers=2, use_gpu=False):\n","    trainer = Trainer(\n","        backend=\"torch\", num_workers=num_workers, use_gpu=use_gpu)\n","    trainer.start()\n","    result = trainer.run(\n","        train_func=train_func,\n","        config={\n","            \"lr\": 1e-3,\n","            \"batch_size\": 128,\n","            \"epochs\": 150,\n","            \"momentum\": 0.9,\n","            \"model_type\": 0,                     # change to 1 for second NN model\n","            \"loss_fn\": nn.CrossEntropyLoss()     # change to nn.nn.NLLLoss() \n","        },\n","        callbacks=[JsonLoggerCallback()])\n","    trainer.shutdown() \n","    return result"],"metadata":{"id":"MJR9hWF1E9R3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["number_of_workers = 2\n","use_gpu = False                              # change to True if using a Ray cluster with GPUs\n","address = \"anyscale://ray_train_ddp_cluster\" # use your anyscale cluster here"],"metadata":{"id":"MzSGbKlHE9UG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ray.init(ignore_reinit_error=True)                           # run locally\n","#ray.init(address=address)                                   # run on a Ray cluster on Anyscale"],"metadata":{"id":"c0wweDT1E9XR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","results = train_fashion_mnist(num_workers=number_of_workers, use_gpu=use_gpu)"],"metadata":{"id":"-aIVInGmE9Yz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"mUaeoBsXFLgg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!grep processor /proc/cpuinfo | wc -l"],"metadata":{"id":"Hu6VfLhtFLi_"},"execution_count":null,"outputs":[]}]}